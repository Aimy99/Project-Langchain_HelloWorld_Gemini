{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOsxg5qNh8lJYFMpX7CWDD/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aimy99/Project-Langchain_HelloWorld_Gemini/blob/main/Project_Langchain_HelloWorld_Gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **PIAIC Quarter 3 - Project 01: 'Langchain_hello_gemini' by Aiman Baquar**"
      ],
      "metadata": {
        "id": "YgqDqW0nkL8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Project 01-Langchain_HelloWorld_Gemini***\n",
        "\n",
        "Integrated Langchain & Gemini API code for fetching responses from the AI model\n",
        "\n",
        "This project is **PIAIC Quarter 3 - Project 01 by Aiman Baquar**, titled **\"Langchain_hello_gemini\"**. It demonstrates the usage of Langchain with Google's Gemini model to build applications powered by Large Language Models (LLMs).\n",
        "\n",
        "**Key Components:**\n",
        "\n",
        "* Langchain Installation: The necessary libraries for Langchain,\n",
        "Langchain_community, and the Google Gemini SDK are installed.\n",
        "\n",
        "* API Key Setup: The project uses an API key to securely connect to the Gemini API for model access.\n",
        "\n",
        "* Model Configuration: Langchain is used to create more complex applications, allowing users to chain LLM calls, integrate external data sources, and create AI-driven agents.\n",
        "\n",
        "Methods:\n",
        "\n",
        "* Method 1: Using Langchain with a prompt to generate responses from the model.\n",
        "* Method 2: Creating a custom prompt template and chaining LLM responses for complex tasks.\n",
        "* Styling: The generated responses are styled in markdown format for better readability."
      ],
      "metadata": {
        "id": "geK3bBMbOExB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Langchain Installation:**"
      ],
      "metadata": {
        "id": "KgUOyb9podYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing Langchain\n",
        "!pip install -q -U langchain\n",
        "!pip install -q -U langchain_community\n",
        "\n",
        "\n",
        "# OpenAI Client for Gemini SDK\n",
        "!pip install -q -U langchain-google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BR4-jEsBkWnH",
        "outputId": "bcf03096-4640-49b5-d74b-d9710fa22fc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**API Key:**"
      ],
      "metadata": {
        "id": "Ou13mlymosaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing userdata from Google Colab to securely store API keys\n",
        "from google.colab import userdata\n",
        "userdata.get('GEMINI_API_KEY')\n",
        "\n",
        "gemini_api_key = userdata.get('GEMINI_API_KEY')\n",
        "\n",
        "# Setting API Key Environment\n",
        "import os\n",
        "\n",
        "# Set the Environment Variable\n",
        "os.environ['GEMINI_API_KEY'] = 'GEMINI_API_KEY'"
      ],
      "metadata": {
        "id": "SqjkVfFNlfG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing required Classes & Libraries:**\n"
      ],
      "metadata": {
        "id": "gVcmuPfOl8Pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing GoogleGenAIChat Class from Google Gemini Langchain module\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Importing AI message class for typing\n",
        "from langchain_core.messages.ai import AIMessage\n",
        "\n",
        "# Chain Message with PromptTemplate\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain"
      ],
      "metadata": {
        "id": "QN5GceTmmUHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Features:**"
      ],
      "metadata": {
        "id": "i9gAEM2Dn0Gc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizing model features\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash-exp\",\n",
        "    api_key= gemini_api_key,\n",
        "    temperature=0.7,\n",
        "    max_tokens= 300,\n",
        "    timeout=None,\n",
        "    max_retries= 6,\n",
        ")"
      ],
      "metadata": {
        "id": "FvhZ39OjnO0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Method 1:**"
      ],
      "metadata": {
        "id": "-X7TIdsJnfMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the LangChain model with a prompt to invoke a response\n",
        "ai_msg: AIMessage = llm.invoke(\"Explain Langchain and why every programmer's first program is Hello World\")\n",
        "\n",
        "# Display Generated Content\n",
        "ai_msg\n",
        "\n",
        "# Display Generated Message\n",
        "print(ai_msg.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lo6dLGwZng6L",
        "outputId": "22b14d06-2f0e-4741-9ff3-2ad50b4e02d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, let's break down Langchain and then discuss the \"Hello World\" tradition.\n",
            "\n",
            "**Langchain: A Framework for Building Powerful Applications with Large Language Models (LLMs)**\n",
            "\n",
            "Imagine you're working with a large language model like GPT-3 or Bard. You can send it a prompt and get a response, which is amazing! But what if you want to do something more complex? What if you want to:\n",
            "\n",
            "* **Chain multiple LLM calls together?** (e.g., summarize a document, then extract key entities, then generate questions based on those entities).\n",
            "* **Integrate external data sources?** (e.g., fetch information from a database or a website and use it in your prompt).\n",
            "* **Add memory to your applications?** (e.g., remember previous interactions in a conversation).\n",
            "* **Create agents that can choose the best tools to solve a problem?** (e.g., using a search engine, a calculator, or other APIs).\n",
            "\n",
            "This is where **Langchain** comes in. It's an open-source framework designed to make building these kinds of complex applications with LLMs much easier. Think of it as a toolkit that provides:\n",
            "\n",
            "* **Components:** Reusable building blocks like LLM wrappers, prompt templates, memory modules, and data loaders.\n",
            "* **Chains:** Ways to link these components together to create workflows that process information step-by-step.\n",
            "*\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Method 2:**"
      ],
      "metadata": {
        "id": "Ze0tujKInpOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a prompt template\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"You are a helpful assistant. Answer the following question:\\n\\n{question}\"\n",
        ")\n",
        "\n",
        "# Create the LLM chain\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "# Run the Chain with a Question\n",
        "question = \"What is LangChain and why this framework is preferred?\"\n",
        "response = chain.run({\"question\": question})\n",
        "\n",
        "# Print AI Response\n",
        "print(\"Answer:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLYfNm-Xnrpe",
        "outputId": "5f4e7dc1-6d1f-4fcf-db2a-395426c63e3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Okay, let's break down LangChain and why it's become so popular in the world of AI and large language models (LLMs).\n",
            "\n",
            "**What is LangChain?**\n",
            "\n",
            "LangChain is an **open-source framework** designed to simplify the development of applications powered by large language models (LLMs) like GPT-3, GPT-4, Bard, and others. Think of it as a toolkit or a set of building blocks that makes it easier for developers to:\n",
            "\n",
            "*   **Chain together different LLM calls:** Instead of just making a single query to an LLM, LangChain allows you to sequence multiple interactions, passing the output of one to the input of another. This is crucial for more complex tasks.\n",
            "*   **Integrate with external data sources:** LLMs are powerful, but they don't know everything. LangChain makes it simple to connect LLMs to databases, APIs, files, and other sources of information, allowing for more informed and context-aware responses.\n",
            "*   **Build agents and tools:** LangChain supports the creation of \"agents\" that can use tools (like search engines, calculators, or custom functions) to accomplish specific tasks. This enables LLMs to go beyond just answering questions and actually perform actions.\n",
            "*   **Handle memory and context:** Maintaining context across multiple interactions with an LLM is crucial for conversational applications. LangChain provides mechanisms for managing conversation history and other forms of memory.\n",
            "*   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Style Message:**"
      ],
      "metadata": {
        "id": "657__A7QqCKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing text-format libraries\n",
        "import textwrap\n",
        "from IPython.display import Markdown\n",
        "\n",
        "# Defining Markdown functions\n",
        "def to_markdown(text) -> Markdown:\n",
        "    text: str = text.replace(\"•\", \"  *\")\n",
        "    return Markdown(textwrap.indent(text, \"> \", predicate=lambda _: True))\n",
        "\n",
        "to_markdown(ai_msg.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "6fIK_pmSqIz_",
        "outputId": "746e5519-d0c2-4888-aad7-87b642db128b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> Okay, let's break down Langchain and then discuss the \"Hello World\" tradition.\n> \n> **Langchain: A Framework for Building Powerful Applications with Large Language Models (LLMs)**\n> \n> Imagine you're working with a large language model like GPT-3 or Bard. You can send it a prompt and get a response, which is amazing! But what if you want to do something more complex? What if you want to:\n> \n> * **Chain multiple LLM calls together?** (e.g., summarize a document, then extract key entities, then generate questions based on those entities).\n> * **Integrate external data sources?** (e.g., fetch information from a database or a website and use it in your prompt).\n> * **Add memory to your applications?** (e.g., remember previous interactions in a conversation).\n> * **Create agents that can choose the best tools to solve a problem?** (e.g., using a search engine, a calculator, or other APIs).\n> \n> This is where **Langchain** comes in. It's an open-source framework designed to make building these kinds of complex applications with LLMs much easier. Think of it as a toolkit that provides:\n> \n> * **Components:** Reusable building blocks like LLM wrappers, prompt templates, memory modules, and data loaders.\n> * **Chains:** Ways to link these components together to create workflows that process information step-by-step.\n> *"
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}